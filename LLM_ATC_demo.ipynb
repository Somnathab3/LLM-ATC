{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Simulation and Quantification of ML-Based Hallucination Effects on Safety Margins in En-route Air Traffic Control"}, {"cell_type": "markdown", "metadata": {}, "source": "This notebook provides a proof-of-concept implementation for the thesis **\"Simulation and Quantification of ML-Based Hallucination Effects on Safety Margins in En-route ATC\"**. It demonstrates data ingestion, feature engineering, a lightweight LLM-based conflict detector, hallucination detection, and integration with the BlueSky-Gym simulator."}, {"cell_type": "markdown", "metadata": {}, "source": "## 1. Environment Setup & Dependencies\nInstall the required libraries. The key packages include:\n- **pandas, numpy, matplotlib, scikit-learn**: data processing and traditional ML helpers.\n- **ollama**: running and fine-tuning lightweight LLMs locally.\n- **unsloth**: simple quantization and PEFT helpers for Llama/Mistral.\n- **bitsandbytes, accelerate**: memory efficient 4-bit quantized training."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!pip install pandas numpy matplotlib scikit-learn ollama unsloth bitsandbytes accelerate"}, {"cell_type": "markdown", "metadata": {}, "source": "## 2. SCAT Dataset Ingestion & Parsing\nDownload the 13-week SCAT JSON archives from Mendeley and unpack them. Each archive contains flight plans, radar plots, predicted trajectories, weather, and airspace information."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import pandas as pd\nfrom pathlib import Path\nimport json\n\ndata_path = Path('data/scat')  # update with the actual path after download\n\n# Example parser for a flight plan JSON file\nfpl_base_files = list(data_path.glob('**/fpl_base.json'))\nall_fpl_base = []\nfor fp in fpl_base_files:\n    with open(fp) as f:\n        all_fpl_base.extend(json.load(f))\n\nfpl_base_df = pd.DataFrame(all_fpl_base)\nfpl_base_df.head()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "Similar parsers can be written for the remaining JSON files such as `fpl_clearance.json`, `I062/105` radar plots (positions & altitudes), `predicted_trajectory.json`, `grib_meteo.json`, and `airspace.json`. The resulting DataFrames provide structured access to the SCAT dataset."}, {"cell_type": "markdown", "metadata": {}, "source": "## 3. Feature Engineering & Conflict Labels\nWe first normalize timestamps and filter to 5-minute windows. For each pair of flights at a given time step we compute horizontal and vertical separation, expressed in nautical miles (NM) and feet. We also compute the time-to-closest-approach (TCA). If the predicted separation within the next five minutes is below the ICAO minima of **5 NM** horizontally or **1000 ft** vertically, the pair is labeled as a conflict."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import numpy as np\n\ndef separation_nm(lat1, lon1, lat2, lon2):\n    R = 3440.065  # Earth radius in nautical miles\n    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n    dphi = np.radians(lat2 - lat1)\n    dl = np.radians(lon2 - lon1)\n    a = np.sin(dphi/2)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dl/2)**2\n    return 2 * R * np.arcsin(np.sqrt(a))\n\n# Example placeholder feature computation\n# df contains positions of two aircraft at time t\n# features = compute_features(df)\n# X, y = build_dataset(features)\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 4. Lightweight LLM Conflict-Detector with Ollama + Unsloth\nWe use Ollama to run a small Llama-2-7B or Mistral-7B model. The model is quantized to 4-bit using Unsloth for minimal resource usage. Fine-tuning is carried out via LoRA/QLoRA."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import torch\nfrom unsloth import FastLanguageModel\nfrom peft import LoraConfig, get_peft_model\n\nmodel, tokenizer = FastLanguageModel.from_pretrained('mistral:latest', load_in_4bit=True)\nconfig = LoraConfig(r=8, lora_alpha=16, target_modules=['q_proj', 'v_proj'])\nmodel = get_peft_model(model, config)\n\n# Training loop placeholder\n# for batch in train_loader:\n#     loss = model(**batch).loss\n#     loss.backward()\n#     optimizer.step()\n#     optimizer.zero_grad()\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Example evaluation placeholder\n# predictions = model.predict(test_features)\n# print(classification_report(test_labels, predictions))\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 5. Hallucination Detection Envelope\nWe compute empirical feature bounds (mean \u00b1 3\u03c3) from the training data. Inputs outside these bounds are considered out-of-distribution (OOD). Whenever the LLM predicts a label for an OOD input, a hallucination flag is recorded."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "feature_means = X_train.mean(axis=0)\nfeature_stds = X_train.std(axis=0)\nlow_bounds = feature_means - 3 * feature_stds\nhigh_bounds = feature_means + 3 * feature_stds\n\ndef is_ood(sample):\n    return ((sample < low_bounds) | (sample > high_bounds)).any()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 6. BlueSky Integration for End-to-End CD&R\nWe integrate the conflict detector with the BlueSky-Gym simulator. The LLM issues conflict alerts two minutes ahead and proposes resolutions (\"Climb +1000 ft\" or \"Turn \u00b120\u00b0\")."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# import gym\n# import bluesky_gym\n# env = bluesky_gym.init('small_sector')\n#\n# for step in range(max_steps):\n#     obs = env.get_state()\n#     conflict = model.predict(obs)\n#     if conflict:\n#         env.apply_clearance('CLIMB', 1000)\n#     env.step()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 7. Metrics & Visualization"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n\n# Placeholder for metrics computation\n# cm = confusion_matrix(y_true, y_pred)\n# disp = ConfusionMatrixDisplay(cm)\n# disp.plot()\n# plt.show()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 8. Distribution Shift Analysis\nWe systematically vary traffic density and measure hallucination rate, safety margin erosion, and reactive workload."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# for density in [1.0, 1.5, 2.0]:\n#     results = run_simulation(density=density)\n#     plot_metrics(results)\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 9. Traceability & Audit Logs\nWe save detailed logs linking raw input, model version, prompts, predicted risk, issued resolutions, and simulation outcomes."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import csv\n\nwith open('audit_log.csv', 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['timestamp', 'model_version', 'prompt', 'prediction', 'resolution', 'outcome', 'hallucination'])\n    # writer.writerow([...])\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## 10. Documentation & Reproducibility\nRun this notebook on a GPU-equipped machine with the required libraries installed. Follow the instructions in each section to reproduce the data processing, model training, and simulation steps. Training data (SCAT) remains private; only the code and the trained model weights are shared here."}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}
