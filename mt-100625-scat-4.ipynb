{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12123646,"sourceType":"datasetVersion","datasetId":7633930}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ML-Based Air Traffic Control: Conflict Detection & Hallucination Analysis\n## Master's Thesis: Simulation and quantification of ML-based hallucination effects on safety margins in enroute control","metadata":{"_uuid":"642a077d-96bb-4d00-b472-a7ed106a853a","_cell_guid":"0fe0320a-06c1-4a5a-b9c0-9724e89ad2ec","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# BlueSky integration\nBLUESKY_AVAILABLE = True\ntry:\n    import bluesky\n    BLUESKY_AVAILABLE = True\n    print(\"âœ… BlueSky available\")\nexcept ImportError:\n    !pip -qq install bluesky","metadata":{"_uuid":"2a3294a5-f239-4fec-91fd-aa2a03f85994","_cell_guid":"18056ecd-f5cd-4f88-8314-e3852662f6a7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-11T09:13:13.928943Z","iopub.execute_input":"2025-06-11T09:13:13.930292Z","iopub.status.idle":"2025-06-11T09:13:21.300880Z","shell.execute_reply.started":"2025-06-11T09:13:13.930241Z","shell.execute_reply":"2025-06-11T09:13:21.299680Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m356.8/356.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.7/66.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport gc\nimport json\nimport warnings\nimport zipfile\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom dataclasses import dataclass\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score\n\n# Transformers and NLP\nfrom transformers import DistilBertTokenizer, DistilBertModel, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)","metadata":{"_uuid":"687c0e67-667a-479a-bf77-29ef2b7dd34f","_cell_guid":"54c53493-dcdd-4698-9747-4c3083b1129b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-11T09:13:21.303262Z","iopub.execute_input":"2025-06-11T09:13:21.303657Z","iopub.status.idle":"2025-06-11T09:14:00.810270Z","shell.execute_reply.started":"2025-06-11T09:13:21.303622Z","shell.execute_reply":"2025-06-11T09:14:00.808868Z"}},"outputs":[{"name":"stderr","text":"2025-06-11 09:13:42.330958: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749633222.654170      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749633222.752755      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Configuration for Thesis Research\n@dataclass\nclass ThesisConfig:\n    \"\"\"Configuration for ML hallucination research in ATC\"\"\"\n    # Data paths\n    scat_data_path: str = \"/kaggle/input/swedish-civil-air-traffic-control-scat-dataset\"\n    output_path: str = \"./thesis_results\"\n    model_save_path: str = \"./thesis_models\"\n    \n    # Model configuration for hallucination analysis\n    model_name: str = \"distilbert-base-uncased\"\n    max_sequence_length: int = 128\n    batch_size: int = 32\n    learning_rate: float = 3e-5\n    num_epochs: int = 5\n    warmup_steps: int = 500\n    \n    # Conflict detection parameters (real operational thresholds)\n    conflict_threshold_nm: float = 5.0  # Nautical miles\n    conflict_threshold_ft: float = 1000.0  # Feet\n    time_horizon_seconds: int = 120  # Look-ahead time\n    \n    # Hallucination detection parameters\n    uncertainty_threshold: float = 0.3\n    mc_dropout_samples: int = 20\n    confidence_threshold: float = 0.8\n    \n    # Training data envelope parameters\n    altitude_min: float = 10000  # ft\n    altitude_max: float = 50000  # ft\n    speed_min: float = 200  # kt\n    speed_max: float = 600  # kt\n    \n    # Device\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nconfig = ThesisConfig()\n\n# Create output directories\nos.makedirs(config.output_path, exist_ok=True)\nos.makedirs(config.model_save_path, exist_ok=True)\n\nprint(f\"ğŸ“ Thesis Configuration loaded. Using device: {config.device}\")\nprint(f\"ğŸ“Š Research Focus: ML Hallucination Quantification in ATC\")","metadata":{"_uuid":"653f4041-42f6-426a-b1c6-fdea51fa046d","_cell_guid":"03c02e61-91e3-4168-b158-7c39fcd29902","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-11T09:14:00.811302Z","iopub.execute_input":"2025-06-11T09:14:00.811965Z","iopub.status.idle":"2025-06-11T09:14:00.826018Z","shell.execute_reply.started":"2025-06-11T09:14:00.811935Z","shell.execute_reply":"2025-06-11T09:14:00.824478Z"}},"outputs":[{"name":"stdout","text":"ğŸ“ Thesis Configuration loaded. Using device: cpu\nğŸ“Š Research Focus: ML Hallucination Quantification in ATC\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Enhanced SCAT Data Loader - Real Data Only\nclass SCATDataLoader:\n    \"\"\"SCAT data loader optimized for thesis research - no synthetic data\"\"\"\n\n    def __init__(self, config: ThesisConfig):\n        self.config = config\n        self.data_path = Path(config.scat_data_path)\n        self.logger = self._setup_logger()\n        self.airspace_data = []\n        self.weather_data = []\n\n    def _setup_logger(self):\n        class SimpleLogger:\n            def info(self, msg): print(f\"â„¹ï¸ {msg}\")\n            def warning(self, msg): print(f\"âš ï¸ {msg}\")\n            def error(self, msg): print(f\"âŒ {msg}\")\n        return SimpleLogger()\n\n    def scan_scat_folders(self) -> List[Path]:\n        \"\"\"Scan for SCAT data sources\"\"\"\n        search_path = self.data_path\n        if not search_path.exists():\n            self.logger.error(f\"SCAT data path does not exist: {search_path}\")\n            return []\n\n        scat_folders = [p for p in search_path.iterdir()\n                        if (p.is_dir() and p.name.lower().startswith(\"scat\")) or\n                           (p.suffix == \".zip\" and p.name.lower().startswith(\"scat\"))]\n\n        self.logger.info(f\"Found {len(scat_folders)} SCAT data sources\")\n        return sorted(scat_folders)\n\n    def load_all_weeks(self, max_weeks: int = 12) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n        \"\"\"Load multiple weeks of real SCAT data\"\"\"\n        week_paths = self.scan_scat_folders()[:max_weeks] if max_weeks else self.scan_scat_folders()\n\n        flights_all, clearances_all, tracks_all = [], [], []\n\n        for wpath in tqdm(week_paths, desc=\"ğŸ“¦ Loading weeks\"):\n            f_df, c_df, t_df = self._load_single_week(wpath)\n            flights_all.append(f_df)\n            clearances_all.append(c_df)\n            tracks_all.append(t_df)\n\n        flights_df = pd.concat(flights_all, ignore_index=True) if flights_all else pd.DataFrame()\n        clearances_df = pd.concat(clearances_all, ignore_index=True) if clearances_all else pd.DataFrame()\n        tracks_df = pd.concat(tracks_all, ignore_index=True) if tracks_all else pd.DataFrame()\n\n        # Clean and validate timestamps\n        tracks_df = self._clean_timestamps(tracks_df)\n        clearances_df = self._clean_timestamps(clearances_df)\n\n        self.logger.info(f\"âœ… Loaded {len(flights_df):,} flights, \"\n                         f\"{len(clearances_df):,} clearances, \"\n                         f\"{len(tracks_df):,} track points from \"\n                         f\"{len(week_paths)} week(s)\")\n        return flights_df, clearances_df, tracks_df\n\n    def _clean_timestamps(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Clean and standardize timestamps\"\"\"\n        if df.empty or 'timestamp' not in df.columns:\n            return df\n        \n        df = df.copy()\n        df['timestamp'] = pd.to_datetime(df['timestamp'], format='mixed', errors='coerce')\n        df = df.dropna(subset=['timestamp'])\n        df = df.sort_values('timestamp').reset_index(drop=True)\n        return df\n\n    def _load_single_week(self, week_path: Path) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n        \"\"\"Load single week of SCAT data\"\"\"\n        week_flights, week_clearances, week_tracks = [], [], []\n\n        if week_path.suffix == \".zip\":\n            with zipfile.ZipFile(week_path, \"r\") as zf:\n                json_files = [f for f in zf.namelist() if f.endswith(\".json\")]\n                for jf in tqdm(json_files, desc=f\"Unzipping {week_path.name}\", leave=False):\n                    try:\n                        with zf.open(jf) as fp:\n                            data = json.load(fp)\n                        self._dispatch_json(jf, data, week_flights, week_clearances, week_tracks)\n                    except Exception as e:\n                        continue\n                self._load_auxiliary_data(zf)\n\n        elif week_path.is_dir():\n            json_paths = list(week_path.glob(\"*.json\"))\n            for jp in tqdm(json_paths, desc=f\"Scanning {week_path.name}\", leave=False):\n                try:\n                    with open(jp) as fp:\n                        data = json.load(fp)\n                    self._dispatch_json(jp.name, data, week_flights, week_clearances, week_tracks)\n                except Exception as e:\n                    continue\n\n        return (pd.DataFrame(week_flights),\n                pd.DataFrame(week_clearances),\n                pd.DataFrame(week_tracks))\n\n    def _dispatch_json(self, name, data, flights, clearances, tracks):\n        \"\"\"Route JSON content appropriately\"\"\"\n        if name in (\"airspace.json\", \"grib_meteo.json\"):\n            (self.airspace_data if \"airspace\" in name else self.weather_data).append(data)\n        else:\n            self._extract_flight_features(data, flights, clearances, tracks)\n\n    def _extract_flight_features(self, flight_data: Dict, flights: list, clearances: list, tracks: list):\n        \"\"\"Extract real flight data features\"\"\"\n        flight_id = flight_data.get('Id') or flight_data.get('id')\n        if not flight_id:\n            return\n        \n        # Extract flight plan data\n        fpl_key = next((k for k in ['Fpl', 'fpl'] if k in flight_data), None)\n        if fpl_key and 'fpl_base' in flight_data[fpl_key]:\n            for base_data in flight_data[fpl_key]['fpl_base']:\n                flight_record = {\n                    'flight_id': flight_id,\n                    'callsign': base_data.get('Callsign') or base_data.get('callsign'),\n                    'aircraft_type': base_data.get('aircraft_type'),\n                    'departure': base_data.get('Adep') or base_data.get('adep'),\n                    'destination': base_data.get('Ades') or base_data.get('ades'),\n                    'timestamp': base_data.get('time_stamp')\n                }\n                flights.append(flight_record)\n        \n        # Extract clearance data\n        if fpl_key and 'fpl_clearance' in flight_data[fpl_key]:\n            for clearance in flight_data[fpl_key]['fpl_clearance']:\n                clearance_record = {\n                    'flight_id': flight_id,\n                    'timestamp': clearance.get('time_stamp'),\n                    'cleared_flight_level': clearance.get('Cfl') or clearance.get('cfl'),\n                    'assigned_speed': clearance.get('assigned_speed_val'),\n                    'assigned_heading': clearance.get('assigned_heading_val')\n                }\n                clearances.append(clearance_record)\n        \n        # Extract track data\n        plots_key = next((k for k in ['Plots', 'plots'] if k in flight_data), None)\n        if plots_key:\n            for plot in flight_data[plots_key]:\n                if 'I062/105' in plot and 'I062/136' in plot:\n                    try:\n                        track_record = {\n                            'flight_id': flight_id,\n                            'timestamp': plot.get('time_of_track'),\n                            'latitude': plot['I062/105'].get('lat'),\n                            'longitude': plot['I062/105'].get('lon'),\n                            'altitude': float(plot['I062/136'].get('measured_flight_level', 0)) * 100\n                        }\n                        \n                        # Add velocity information if available\n                        if 'I062/185' in plot:\n                            vx = plot['I062/185'].get('vx', 0)\n                            vy = plot['I062/185'].get('vy', 0)\n                            track_record['ground_speed'] = np.sqrt(vx**2 + vy**2) * 1.94384  # m/s to kt\n                            track_record['heading'] = np.degrees(np.arctan2(vx, vy)) % 360\n                        \n                        # Validate data is within operational envelope\n                        if self._validate_track_data(track_record):\n                            tracks.append(track_record)\n                    except (ValueError, TypeError):\n                        continue\n\n    def _validate_track_data(self, track: Dict) -> bool:\n        \"\"\"Validate track data is within operational envelope\"\"\"\n        try:\n            altitude = track.get('altitude', 0)\n            speed = track.get('ground_speed', 0)\n            lat = track.get('latitude', 0)\n            lon = track.get('longitude', 0)\n            \n            # Check operational envelope\n            if not (self.config.altitude_min <= altitude <= self.config.altitude_max):\n                return False\n            if not (self.config.speed_min <= speed <= self.config.speed_max):\n                return False\n            if not (-90 <= lat <= 90 and -180 <= lon <= 180):\n                return False\n            \n            return True\n        except (ValueError, TypeError):\n            return False\n\n    def _load_auxiliary_data(self, zip_ref):\n        \"\"\"Load auxiliary airspace and weather data\"\"\"\n        for aux in (\"airspace.json\", \"grib_meteo.json\"):\n            try:\n                if aux in zip_ref.namelist():\n                    with zip_ref.open(aux) as f:\n                        data = json.load(f)\n                        (self.airspace_data if \"airspace\" in aux else self.weather_data).append(data)\n            except Exception:\n                pass","metadata":{"_uuid":"2238a2a4-452b-484b-9d62-a351503abc8d","_cell_guid":"3f9ee4ae-81db-438b-91d3-804680129f8d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-11T09:14:00.829662Z","iopub.execute_input":"2025-06-11T09:14:00.830044Z","iopub.status.idle":"2025-06-11T09:14:00.913994Z","shell.execute_reply.started":"2025-06-11T09:14:00.830018Z","shell.execute_reply":"2025-06-11T09:14:00.912645Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Real Conflict Detection - No Synthetic Data\nclass RealConflictDetector:\n    \"\"\"Detect real conflicts from operational data only\"\"\"\n    \n    def __init__(self, config: ThesisConfig):\n        self.config = config\n        self.conflict_threshold_nm = config.conflict_threshold_nm\n        self.conflict_threshold_ft = config.conflict_threshold_ft\n        self.time_horizon = config.time_horizon_seconds\n    \n    def haversine_distance(self, lat1, lon1, lat2, lon2):\n        \"\"\"Calculate distance between two points in nautical miles\"\"\"\n        R = 3440.065  # Earth radius in nautical miles\n        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n        dlat = lat2 - lat1\n        dlon = lon2 - lon1\n        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n        c = 2 * np.arcsin(np.sqrt(a))\n        return R * c\n    \n    def detect_real_conflicts(self, tracks_df: pd.DataFrame, clearances_df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Detect real conflicts from operational data - no synthetic generation\"\"\"\n        if tracks_df.empty:\n            print(\"No track data available for conflict detection\")\n            return pd.DataFrame()\n        \n        conflicts = []\n        tracks_df = tracks_df.copy()\n        \n        # Ensure timestamps are datetime\n        if not pd.api.types.is_datetime64_any_dtype(tracks_df['timestamp']):\n            tracks_df['timestamp'] = pd.to_datetime(tracks_df['timestamp'], errors='coerce')\n        \n        tracks_df = tracks_df.dropna(subset=['timestamp', 'latitude', 'longitude', 'altitude'])\n        \n        if tracks_df.empty:\n            print(\"No valid track data after cleaning\")\n            return pd.DataFrame()\n        \n        # Sample time windows for analysis\n        unique_times = tracks_df['timestamp'].unique()\n        sample_size = min(len(unique_times), 2000)  # Reasonable sample for real analysis\n        sampled_times = np.random.choice(unique_times, size=sample_size, replace=False)\n        \n        print(f\"ğŸ” Analyzing {sample_size} time windows for real conflicts\")\n        \n        for time_sample in tqdm(sampled_times, desc=\"Detecting real conflicts\"):\n            # Get aircraft at this time\n            current_aircraft = tracks_df[tracks_df['timestamp'] == time_sample]\n            \n            if len(current_aircraft) < 2:\n                continue\n            \n            # Group by flight to handle multiple positions\n            aircraft_groups = current_aircraft.groupby('flight_id').last().reset_index()\n            \n            if len(aircraft_groups) < 2:\n                continue\n            \n            # Check all pairs for conflicts\n            for i in range(len(aircraft_groups)):\n                for j in range(i+1, len(aircraft_groups)):\n                    ac1 = aircraft_groups.iloc[i]\n                    ac2 = aircraft_groups.iloc[j]\n                    \n                    # Calculate current separation\n                    h_dist = self.haversine_distance(\n                        ac1['latitude'], ac1['longitude'],\n                        ac2['latitude'], ac2['longitude']\n                    )\n                    v_dist = abs(ac1['altitude'] - ac2['altitude'])\n                    \n                    # Check if this is a real conflict (within thresholds)\n                    if h_dist < self.conflict_threshold_nm and v_dist < self.conflict_threshold_ft:\n                        # This is a real conflict - check for associated clearances\n                        clearance_info = self._find_associated_clearances(\n                            time_sample, [ac1['flight_id'], ac2['flight_id']], clearances_df\n                        )\n                        \n                        conflict_record = {\n                            'timestamp': time_sample,\n                            'flight_id_1': ac1['flight_id'],\n                            'flight_id_2': ac2['flight_id'],\n                            'horizontal_distance': h_dist,\n                            'vertical_distance': v_dist,\n                            'conflict': True,\n                            'clearance_issued': clearance_info['issued'],\n                            'clearance_type': clearance_info['type'],\n                            'altitude_1': ac1['altitude'],\n                            'altitude_2': ac2['altitude'],\n                            'speed_1': ac1.get('ground_speed', 450),\n                            'speed_2': ac2.get('ground_speed', 450),\n                            'heading_1': ac1.get('heading', 90),\n                            'heading_2': ac2.get('heading', 90)\n                        }\n                        conflicts.append(conflict_record)\n        \n        conflicts_df = pd.DataFrame(conflicts)\n        print(f\"ğŸš¨ Found {len(conflicts_df)} real conflicts\")\n        \n        # Add non-conflict samples from same operational data\n        non_conflicts = self._extract_real_non_conflicts(tracks_df, len(conflicts_df))\n        \n        # Combine real conflicts and non-conflicts\n        all_scenarios = pd.concat([conflicts_df, non_conflicts], ignore_index=True)\n        \n        return all_scenarios\n    \n    def _find_associated_clearances(self, timestamp: pd.Timestamp, flight_ids: List[str], \n                                   clearances_df: pd.DataFrame) -> Dict:\n        \"\"\"Find clearances associated with a conflict\"\"\"\n        if clearances_df.empty:\n            return {'issued': False, 'type': 'none'}\n        \n        # Look for clearances within time window\n        time_window = pd.Timedelta(minutes=5)\n        clearance_mask = (\n            (clearances_df['timestamp'] >= timestamp - time_window) &\n            (clearances_df['timestamp'] <= timestamp + time_window) &\n            (clearances_df['flight_id'].isin(flight_ids))\n        )\n        \n        relevant_clearances = clearances_df[clearance_mask]\n        \n        if len(relevant_clearances) == 0:\n            return {'issued': False, 'type': 'none'}\n        \n        # Determine clearance type\n        clearance_type = self._classify_clearance_type(relevant_clearances)\n        return {'issued': True, 'type': clearance_type}\n    \n    def _classify_clearance_type(self, clearances: pd.DataFrame) -> str:\n        \"\"\"Classify the type of clearance\"\"\"\n        if clearances['cleared_flight_level'].notna().any():\n            return \"altitude_change\"\n        elif clearances['assigned_heading'].notna().any():\n            return \"heading_change\"\n        elif clearances['assigned_speed'].notna().any():\n            return \"speed_change\"\n        else:\n            return \"other\"\n    \n    def _extract_real_non_conflicts(self, tracks_df: pd.DataFrame, num_conflicts: int) -> pd.DataFrame:\n        \"\"\"Extract real non-conflict scenarios from operational data\"\"\"\n        non_conflicts = []\n        \n        # Sample different time windows\n        unique_times = tracks_df['timestamp'].unique()\n        sample_times = np.random.choice(unique_times, size=min(len(unique_times), num_conflicts * 2), replace=False)\n        \n        for time_sample in sample_times:\n            current_aircraft = tracks_df[tracks_df['timestamp'] == time_sample]\n            aircraft_groups = current_aircraft.groupby('flight_id').last().reset_index()\n            \n            if len(aircraft_groups) < 2:\n                continue\n            \n            # Find pairs with safe separation\n            for i in range(len(aircraft_groups)):\n                for j in range(i+1, len(aircraft_groups)):\n                    ac1 = aircraft_groups.iloc[i]\n                    ac2 = aircraft_groups.iloc[j]\n                    \n                    h_dist = self.haversine_distance(\n                        ac1['latitude'], ac1['longitude'],\n                        ac2['latitude'], ac2['longitude']\n                    )\n                    v_dist = abs(ac1['altitude'] - ac2['altitude'])\n                    \n                    # Non-conflict: safe separation\n                    if h_dist > self.conflict_threshold_nm * 1.5 or v_dist > self.conflict_threshold_ft * 1.5:\n                        non_conflict_record = {\n                            'timestamp': time_sample,\n                            'flight_id_1': ac1['flight_id'],\n                            'flight_id_2': ac2['flight_id'],\n                            'horizontal_distance': h_dist,\n                            'vertical_distance': v_dist,\n                            'conflict': False,\n                            'clearance_issued': False,\n                            'clearance_type': 'none',\n                            'altitude_1': ac1['altitude'],\n                            'altitude_2': ac2['altitude'],\n                            'speed_1': ac1.get('ground_speed', 450),\n                            'speed_2': ac2.get('ground_speed', 450),\n                            'heading_1': ac1.get('heading', 90),\n                            'heading_2': ac2.get('heading', 90)\n                        }\n                        non_conflicts.append(non_conflict_record)\n                        \n                        if len(non_conflicts) >= num_conflicts:\n                            break\n                \n                if len(non_conflicts) >= num_conflicts:\n                    break\n                    \n            if len(non_conflicts) >= num_conflicts:\n                break\n        \n        return pd.DataFrame(non_conflicts)","metadata":{"_uuid":"bb4a6acc-b75c-4166-972f-d1f46eeaacc9","_cell_guid":"ae9b450a-f49a-4ba4-b932-50a2462878f5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-11T09:14:00.915639Z","iopub.execute_input":"2025-06-11T09:14:00.916071Z","iopub.status.idle":"2025-06-11T09:14:00.948757Z","shell.execute_reply.started":"2025-06-11T09:14:00.916032Z","shell.execute_reply":"2025-06-11T09:14:00.947488Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Hallucination-Aware Dataset for Thesis\nclass HallucinationATCDataset(Dataset):\n    \"\"\"Dataset designed for hallucination analysis in ATC ML models\"\"\"\n    \n    def __init__(self, scenarios_df: pd.DataFrame, tokenizer, max_length: int = 128):\n        self.scenarios_df = scenarios_df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n        # Encode clearance types\n        self.clearance_encoder = LabelEncoder()\n        self.scenarios_df['clearance_label'] = self.clearance_encoder.fit_transform(\n            self.scenarios_df['clearance_type'].fillna('none')\n        )\n        \n        # Add envelope indicators for hallucination analysis\n        self.scenarios_df['outside_training_envelope'] = self._detect_envelope_violations()\n    \n    def _detect_envelope_violations(self) -> pd.Series:\n        \"\"\"Detect scenarios outside typical training envelope\"\"\"\n        violations = (\n            (self.scenarios_df['altitude_1'] < config.altitude_min) |\n            (self.scenarios_df['altitude_1'] > config.altitude_max) |\n            (self.scenarios_df['altitude_2'] < config.altitude_min) |\n            (self.scenarios_df['altitude_2'] > config.altitude_max) |\n            (self.scenarios_df['speed_1'] < config.speed_min) |\n            (self.scenarios_df['speed_1'] > config.speed_max) |\n            (self.scenarios_df['speed_2'] < config.speed_min) |\n            (self.scenarios_df['speed_2'] > config.speed_max)\n        )\n        return violations\n    \n    def __len__(self):\n        return len(self.scenarios_df)\n    \n    def __getitem__(self, idx):\n        row = self.scenarios_df.iloc[idx]\n        \n        # Create scenario description\n        text = self._create_scenario_text(row)\n        \n        # Tokenize\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'conflict_label': torch.tensor(1 if row['conflict'] else 0, dtype=torch.long),\n            'clearance_label': torch.tensor(row['clearance_label'], dtype=torch.long),\n            'envelope_violation': torch.tensor(row['outside_training_envelope'], dtype=torch.float)\n        }\n    \n    def _create_scenario_text(self, row):\n        \"\"\"Create natural language scenario description\"\"\"\n        text = (\n            f\"Aircraft A at FL{int(row['altitude_1']/100):03d} \"\n            f\"heading {int(row['heading_1']):03d}Â° \"\n            f\"speed {int(row['speed_1']):03d} kt; \"\n            f\"Aircraft B at FL{int(row['altitude_2']/100):03d} \"\n            f\"heading {int(row['heading_2']):03d}Â° \"\n            f\"speed {int(row['speed_2']):03d} kt; \"\n            f\"horizontal separation {row['horizontal_distance']:.1f} NM; \"\n            f\"vertical separation {row['vertical_distance']:.0f} ft.\"\n        )\n        return text","metadata":{"_uuid":"0648065d-0e79-4787-b65d-02e355efe2f8","_cell_guid":"cd58d0c5-0fd5-4b4c-8c2e-f09faaca088e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-11T09:14:00.950460Z","iopub.execute_input":"2025-06-11T09:14:00.950882Z","iopub.status.idle":"2025-06-11T09:14:00.979071Z","shell.execute_reply.started":"2025-06-11T09:14:00.950854Z","shell.execute_reply":"2025-06-11T09:14:00.977373Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Hallucination-Aware Model Architecture\nclass HallucinationAwareATCModel(nn.Module):\n    \"\"\"ATC model with built-in hallucination detection capabilities\"\"\"\n    \n    def __init__(self, model_name: str, num_clearance_types: int, dropout_rate: float = 0.1):\n        super().__init__()\n        \n        self.bert = DistilBertModel.from_pretrained(model_name)\n        hidden_size = self.bert.config.hidden_size\n        \n        # Main task heads\n        self.conflict_head = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_size // 2, 2)\n        )\n        \n        self.resolution_head = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_size // 2, num_clearance_types)\n        )\n        \n        # Hallucination detection head\n        self.hallucination_head = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size // 4),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(hidden_size // 4, 1),\n            nn.Sigmoid()\n        )\n        \n        self.dropout_rate = dropout_rate\n    \n    def forward(self, input_ids, attention_mask, enable_dropout=False):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0]\n        \n        if enable_dropout:\n            pooled_output = F.dropout(pooled_output, p=self.dropout_rate, training=True)\n        \n        conflict_logits = self.conflict_head(pooled_output)\n        resolution_logits = self.resolution_head(pooled_output)\n        hallucination_score = self.hallucination_head(pooled_output)\n        \n        return conflict_logits, resolution_logits, hallucination_score\n    \n    def get_uncertainty_estimates(self, input_ids, attention_mask, n_samples=20):\n        \"\"\"Estimate model uncertainty using Monte Carlo dropout\"\"\"\n        self.train()  # Enable dropout\n        \n        conflict_samples = []\n        resolution_samples = []\n        hallucination_samples = []\n        \n        with torch.no_grad():\n            for _ in range(n_samples):\n                conflict_logits, resolution_logits, hallucination_score = self.forward(\n                    input_ids, attention_mask, enable_dropout=True\n                )\n                conflict_samples.append(F.softmax(conflict_logits, dim=-1))\n                resolution_samples.append(F.softmax(resolution_logits, dim=-1))\n                hallucination_samples.append(hallucination_score)\n        \n        # Calculate statistics\n        conflict_probs = torch.stack(conflict_samples)\n        resolution_probs = torch.stack(resolution_samples)\n        hallucination_probs = torch.stack(hallucination_samples)\n        \n        results = {\n            'conflict_mean': conflict_probs.mean(dim=0),\n            'conflict_std': conflict_probs.std(dim=0),\n            'conflict_entropy': -torch.sum(conflict_probs.mean(dim=0) * torch.log(conflict_probs.mean(dim=0) + 1e-8), dim=-1),\n            'resolution_mean': resolution_probs.mean(dim=0),\n            'resolution_std': resolution_probs.std(dim=0),\n            'hallucination_mean': hallucination_probs.mean(dim=0),\n            'hallucination_std': hallucination_probs.std(dim=0)\n        }\n        \n        return results","metadata":{"_uuid":"97da05c7-dae4-4464-bfa1-9b1e5cf3c3ee","_cell_guid":"a13f694c-11fc-44af-8154-8e49df840eb2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-11T09:14:00.980726Z","iopub.execute_input":"2025-06-11T09:14:00.981330Z","iopub.status.idle":"2025-06-11T09:14:01.011582Z","shell.execute_reply.started":"2025-06-11T09:14:00.981282Z","shell.execute_reply":"2025-06-11T09:14:01.010183Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Training Pipeline with Hallucination Loss\nclass HallucinationAwareTrainer:\n    \"\"\"Training pipeline incorporating hallucination detection\"\"\"\n    \n    def __init__(self, model, train_loader, val_loader, config):\n        self.model = model.to(config.device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.config = config\n        \n        self.optimizer = AdamW(model.parameters(), lr=config.learning_rate, weight_decay=1e-5)\n        total_steps = len(train_loader) * config.num_epochs\n        self.scheduler = get_linear_schedule_with_warmup(\n            self.optimizer, num_warmup_steps=config.warmup_steps, num_training_steps=total_steps\n        )\n        \n        # Loss functions\n        self.conflict_loss_fn = nn.CrossEntropyLoss()\n        self.resolution_loss_fn = nn.CrossEntropyLoss()\n        self.hallucination_loss_fn = nn.BCELoss()\n        \n        # Loss weights\n        self.conflict_weight = 1.0\n        self.resolution_weight = 1.0\n        self.hallucination_weight = 0.5\n    \n    def train(self):\n        \"\"\"Training loop with hallucination awareness\"\"\"\n        for epoch in range(self.config.num_epochs):\n            self.model.train()\n            total_loss = 0\n            \n            for batch in tqdm(self.train_loader, desc=f\"Epoch {epoch+1}\"):\n                # Move to device\n                input_ids = batch['input_ids'].to(self.config.device)\n                attention_mask = batch['attention_mask'].to(self.config.device)\n                conflict_labels = batch['conflict_label'].to(self.config.device)\n                clearance_labels = batch['clearance_label'].to(self.config.device)\n                envelope_violations = batch['envelope_violation'].to(self.config.device)\n                \n                # Forward pass\n                conflict_logits, resolution_logits, hallucination_scores = self.model(\n                    input_ids, attention_mask\n                )\n                \n                # Calculate losses\n                conflict_loss = self.conflict_loss_fn(conflict_logits, conflict_labels)\n                resolution_loss = self.resolution_loss_fn(resolution_logits, clearance_labels)\n                hallucination_loss = self.hallucination_loss_fn(\n                    hallucination_scores.squeeze(), envelope_violations\n                )\n                \n                # Combined loss\n                total_batch_loss = (\n                    self.conflict_weight * conflict_loss +\n                    self.resolution_weight * resolution_loss +\n                    self.hallucination_weight * hallucination_loss\n                )\n                \n                # Backward pass\n                total_batch_loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                self.optimizer.step()\n                self.scheduler.step()\n                self.optimizer.zero_grad()\n                \n                total_loss += total_batch_loss.item()\n            \n            avg_loss = total_loss / len(self.train_loader)\n            print(f\"Epoch {epoch+1}/{self.config.num_epochs}, Average Loss: {avg_loss:.4f}\")\n            \n            # Validation\n            self.validate()\n    \n    def validate(self):\n        \"\"\"Validation with hallucination analysis\"\"\"\n        self.model.eval()\n        total_loss = 0\n        \n        conflict_preds, conflict_labels = [], []\n        clearance_preds, clearance_labels = [], []\n        hallucination_preds, hallucination_labels = [], []\n        \n        with torch.no_grad():\n            for batch in self.val_loader:\n                input_ids = batch['input_ids'].to(self.config.device)\n                attention_mask = batch['attention_mask'].to(self.config.device)\n                conflict_true = batch['conflict_label'].to(self.config.device)\n                clearance_true = batch['clearance_label'].to(self.config.device)\n                envelope_true = batch['envelope_violation'].to(self.config.device)\n                \n                conflict_logits, resolution_logits, hallucination_scores = self.model(\n                    input_ids, attention_mask\n                )\n                \n                # Calculate losses\n                conflict_loss = self.conflict_loss_fn(conflict_logits, conflict_true)\n                resolution_loss = self.resolution_loss_fn(resolution_logits, clearance_true)\n                hallucination_loss = self.hallucination_loss_fn(\n                    hallucination_scores.squeeze(), envelope_true\n                )\n                \n                total_loss += conflict_loss.item() + resolution_loss.item() + hallucination_loss.item()\n                \n                # Collect predictions\n                conflict_preds.extend(torch.argmax(conflict_logits, dim=1).cpu().numpy())\n                conflict_labels.extend(conflict_true.cpu().numpy())\n                clearance_preds.extend(torch.argmax(resolution_logits, dim=1).cpu().numpy())\n                clearance_labels.extend(clearance_true.cpu().numpy())\n                hallucination_preds.extend((hallucination_scores.squeeze() > 0.5).cpu().numpy())\n                hallucination_labels.extend(envelope_true.cpu().numpy())\n        \n        print(f\"Validation Loss: {total_loss / len(self.val_loader):.4f}\")\n        print(\"\\nğŸ“Š Conflict Detection Report:\")\n        print(classification_report(conflict_labels, conflict_preds))\n        print(\"\\nğŸ“Š Clearance Prediction Report:\")\n        print(classification_report(clearance_labels, clearance_preds))\n        print(\"\\nğŸ“Š Hallucination Detection Report:\")\n        print(classification_report(hallucination_labels, hallucination_preds))","metadata":{"_uuid":"3707988c-a67b-4810-9fa7-40e8906321b6","_cell_guid":"5dceb1b8-9908-4189-83f2-a20d24244655","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-11T09:14:01.013100Z","iopub.execute_input":"2025-06-11T09:14:01.013624Z","iopub.status.idle":"2025-06-11T09:14:01.047450Z","shell.execute_reply.started":"2025-06-11T09:14:01.013582Z","shell.execute_reply":"2025-06-11T09:14:01.046098Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Fixed BlueSky Integration and Visualization\nclass EnhancedBlueSkySimulator:\n    \"\"\"Enhanced BlueSky simulator with proper timestamp handling\"\"\"\n    \n    def __init__(self):\n        self.state = {}\n        self.detector = RealConflictDetector(config)\n        print(\"âœˆï¸ Enhanced BlueSky Simulator initialized\")\n    \n    def update_state(self, tracks_df: pd.DataFrame):\n        \"\"\"Update simulator state with validated track data\"\"\"\n        if not tracks_df.empty:\n            # Ensure proper timestamp format\n            tracks_df = tracks_df.copy()\n            if not pd.api.types.is_datetime64_any_dtype(tracks_df['timestamp']):\n                tracks_df['timestamp'] = pd.to_datetime(tracks_df['timestamp'], errors='coerce')\n            tracks_df = tracks_df.dropna(subset=['timestamp'])\n            \n        self.state['tracks'] = tracks_df\n        print(f\"ğŸ“¡ Simulator updated with {len(tracks_df)} track points\")\n    \n    def get_separation(self, flight_id_1: str, flight_id_2: str) -> Tuple[float, float]:\n        \"\"\"Get current separation between aircraft\"\"\"\n        if 'tracks' not in self.state or self.state['tracks'].empty:\n            return 0.0, 0.0\n        \n        tracks = self.state['tracks']\n        ac1_tracks = tracks[tracks['flight_id'] == flight_id_1]\n        ac2_tracks = tracks[tracks['flight_id'] == flight_id_2]\n        \n        if ac1_tracks.empty or ac2_tracks.empty:\n            return 0.0, 0.0\n        \n        # Get latest positions\n        ac1 = ac1_tracks.iloc[-1]\n        ac2 = ac2_tracks.iloc[-1]\n        \n        h_dist = self.detector.haversine_distance(\n            ac1['latitude'], ac1['longitude'], \n            ac2['latitude'], ac2['longitude']\n        )\n        v_dist = abs(ac1.get('altitude', 0) - ac2.get('altitude', 0))\n        \n        return h_dist, v_dist\n    \n    def validate_clearance(self, flight_id_1: str, flight_id_2: str, clearance_type: str) -> bool:\n        \"\"\"Validate if clearance maintains safe separation\"\"\"\n        h_dist, v_dist = self.get_separation(flight_id_1, flight_id_2)\n        safe = h_dist > config.conflict_threshold_nm or v_dist > config.conflict_threshold_ft\n        print(f\"ğŸ” Clearance validation: H={h_dist:.1f}NM, V={v_dist:.0f}ft, Safe={safe}\")\n        return safe\n\ndef plot_trajectories(simulator):\n    \"\"\"Plot aircraft trajectories with proper error handling\"\"\"\n    if 'tracks' not in simulator.state or simulator.state['tracks'].empty:\n        print(\"âš ï¸ No tracks available for plotting\")\n        return\n    \n    tracks_df = simulator.state['tracks']\n    \n    plt.figure(figsize=(14, 10))\n    flight_ids = tracks_df['flight_id'].unique()[:10]  # Limit for readability\n    \n    for flight_id in flight_ids:\n        flight_tracks = tracks_df[tracks_df['flight_id'] == flight_id].sort_values('timestamp')\n        if len(flight_tracks) > 1:\n            plt.plot(flight_tracks['longitude'], flight_tracks['latitude'], \n                    marker='o', markersize=2, alpha=0.7, label=f'Flight {flight_id}')\n    \n    plt.xlabel('Longitude (Â°)')\n    plt.ylabel('Latitude (Â°)')\n    plt.title('Real Aircraft Trajectories from SCAT Data')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.savefig(os.path.join(config.output_path, 'real_trajectories.png'), dpi=300, bbox_inches='tight')\n    plt.close()\n    print(\"ğŸ“ˆ Trajectory plot saved\")\n\ndef plot_separation_over_time(simulator, pair: Tuple[str, str]):\n    \"\"\"Plot separation with fixed timestamp handling\"\"\"\n    if 'tracks' not in simulator.state or simulator.state['tracks'].empty:\n        print(\"âš ï¸ No tracks available for separation plotting\")\n        return\n    \n    tracks_df = simulator.state['tracks']\n    ac1_tracks = tracks_df[tracks_df['flight_id'] == pair[0]].copy()\n    ac2_tracks = tracks_df[tracks_df['flight_id'] == pair[1]].copy()\n    \n    if ac1_tracks.empty or ac2_tracks.empty:\n        print(f\"âš ï¸ No data for aircraft pair {pair}\")\n        return\n    \n    # Ensure timestamps are properly formatted\n    ac1_tracks['timestamp'] = pd.to_datetime(ac1_tracks['timestamp'], errors='coerce')\n    ac2_tracks['timestamp'] = pd.to_datetime(ac2_tracks['timestamp'], errors='coerce')\n    \n    ac1_tracks = ac1_tracks.dropna(subset=['timestamp']).sort_values('timestamp')\n    ac2_tracks = ac2_tracks.dropna(subset=['timestamp']).sort_values('timestamp')\n    \n    if ac1_tracks.empty or ac2_tracks.empty:\n        print(f\"âš ï¸ No valid timestamps for pair {pair}\")\n        return\n    \n    # Create time-aligned data using merge_asof with proper datetime index\n    try:\n        merged = pd.merge_asof(\n            ac1_tracks[['timestamp', 'latitude', 'longitude', 'altitude']],\n            ac2_tracks[['timestamp', 'latitude', 'longitude', 'altitude']],\n            on='timestamp',\n            suffixes=('_1', '_2'),\n            direction='nearest'\n        )\n        \n        if merged.empty:\n            print(f\"âš ï¸ No aligned data for pair {pair}\")\n            return\n        \n        # Calculate separations\n        detector = RealConflictDetector(config)\n        merged['h_dist'] = merged.apply(\n            lambda row: detector.haversine_distance(\n                row['latitude_1'], row['longitude_1'],\n                row['latitude_2'], row['longitude_2']\n            ), axis=1\n        )\n        merged['v_dist'] = abs(merged['altitude_1'] - merged['altitude_2'])\n        \n        # Plot\n        plt.figure(figsize=(14, 8))\n        plt.subplot(2, 1, 1)\n        plt.plot(merged['timestamp'], merged['h_dist'], 'b-', linewidth=2, label='Horizontal Distance')\n        plt.axhline(y=config.conflict_threshold_nm, color='r', linestyle='--', \n                   label=f'Conflict Threshold ({config.conflict_threshold_nm} NM)')\n        plt.ylabel('Distance (NM)')\n        plt.title(f'Separation Analysis: {pair[0]} vs {pair[1]}')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        \n        plt.subplot(2, 1, 2)\n        plt.plot(merged['timestamp'], merged['v_dist']/1000, 'g-', linewidth=2, label='Vertical Distance')\n        plt.axhline(y=config.conflict_threshold_ft/1000, color='r', linestyle='--', \n                   label=f'Conflict Threshold ({config.conflict_threshold_ft/1000:.1f} kft)')\n        plt.xlabel('Time')\n        plt.ylabel('Distance (kft)')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(config.output_path, f'separation_{pair[0]}_{pair[1]}.png'), \n                   dpi=300, bbox_inches='tight')\n        plt.close()\n        print(f\"ğŸ“Š Separation plot saved for {pair}\")\n        \n    except Exception as e:\n        print(f\"âŒ Error plotting separation for {pair}: {e}\")","metadata":{"_uuid":"97a94891-6069-4075-ae93-e5afb48b2993","_cell_guid":"6d0e3100-0b06-4df5-9ebd-8e35caa88c5c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-11T09:14:01.048740Z","iopub.execute_input":"2025-06-11T09:14:01.049131Z","iopub.status.idle":"2025-06-11T09:14:01.079560Z","shell.execute_reply.started":"2025-06-11T09:14:01.049103Z","shell.execute_reply":"2025-06-11T09:14:01.078355Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Main Research Pipeline\ndef main():\n    \"\"\"Main pipeline for thesis research on ML hallucination in ATC\"\"\"\n    print(\"ğŸ“ Starting ML Hallucination Research Pipeline\")\n    print(\"=\" * 60)\n    \n    # Load real SCAT data\n    print(\"ğŸ“¦ Loading SCAT Data...\")\n    data_loader = SCATDataLoader(config)\n    flights_df, clearances_df, tracks_df = data_loader.load_all_weeks(max_weeks=3)\n    \n    if tracks_df.empty:\n        print(\"âŒ No track data loaded. Cannot proceed with analysis.\")\n        return\n    \n    # Detect real conflicts only\n    print(\"ğŸ” Detecting Real Conflicts...\")\n    detector = RealConflictDetector(config)\n    scenarios_df = detector.detect_real_conflicts(tracks_df, clearances_df)\n    \n    if scenarios_df.empty:\n        print(\"âŒ No scenarios generated. Cannot proceed with training.\")\n        return\n    \n    print(f\"âœ… Generated {len(scenarios_df)} real scenarios ({scenarios_df['conflict'].sum()} conflicts)\")\n    \n    # Prepare ML pipeline\n    print(\"ğŸ¤– Initializing ML Pipeline...\")\n    tokenizer = DistilBertTokenizer.from_pretrained(config.model_name)\n    \n    # Create dataset\n    dataset = HallucinationATCDataset(scenarios_df, tokenizer, config.max_sequence_length)\n    \n    # Split data\n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=config.batch_size)\n    \n    # Initialize model\n    num_clearance_types = scenarios_df['clearance_type'].nunique()\n    model = HallucinationAwareATCModel(config.model_name, num_clearance_types)\n    \n    # Train model\n    print(\"ğŸ¯ Training Hallucination-Aware Model...\")\n    trainer = HallucinationAwareTrainer(model, train_loader, val_loader, config)\n    trainer.train()\n    \n    # Save model\n    model_path = os.path.join(config.model_save_path, 'hallucination_aware_atc_model.pt')\n    torch.save(model.state_dict(), model_path)\n    print(f\"ğŸ’¾ Model saved to {model_path}\")\n    \n    # Initialize simulator and test\n    print(\"âœˆï¸ Testing with BlueSky Simulator...\")\n    simulator = EnhancedBlueSkySimulator()\n    simulator.update_state(tracks_df)\n    \n    # Test validation on real conflicts\n    if not scenarios_df[scenarios_df['conflict']].empty:\n        sample_conflict = scenarios_df[scenarios_df['conflict']].iloc[0]\n        flight_pair = (sample_conflict['flight_id_1'], sample_conflict['flight_id_2'])\n        \n        clearance_valid = simulator.validate_clearance(\n            flight_pair[0], flight_pair[1], sample_conflict['clearance_type']\n        )\n        print(f\"âœ… Clearance validation result: {clearance_valid}\")\n        \n        # Generate visualizations\n        print(\"ğŸ“Š Generating Visualizations...\")\n        plot_trajectories(simulator)\n        plot_separation_over_time(simulator, flight_pair)\n    \n    print(\"ğŸ“ Research pipeline completed successfully!\")\n    print(f\"ğŸ“ Results saved to: {config.output_path}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"5e9a3168-3db4-4b38-9328-1055137747b2","_cell_guid":"70e4e25e-6042-43eb-bbe8-9d10a4da9758","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-11T09:14:01.082926Z","iopub.execute_input":"2025-06-11T09:14:01.083475Z"}},"outputs":[{"name":"stdout","text":"ğŸ“ Starting ML Hallucination Research Pipeline\n============================================================\nğŸ“¦ Loading SCAT Data...\nâ„¹ï¸ Found 13 SCAT data sources\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"ğŸ“¦ Loading weeks:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dca7a9510b5b40878b8ecf89c563d257"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Scanning scat20161015_20161021:   0%|          | 0/13140 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Scanning scat20161112_20161118:   0%|          | 0/12250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f6e6a2f7df24f2bb29be839443d513d"}},"metadata":{}}],"execution_count":null}]}